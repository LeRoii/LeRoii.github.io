---
layout: post
title:  "ML-note"
date:   2021-04-27 10:16:26 +0800
categories: jekyll update
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
## softmax
softmax又称为归一化指数函数，目的是将多分类的结果以概率的形式展现出来，  
将每个节点的输入映射到0-1之间，且输出的值和为1  
使用指数函数保证输出结果非负，归一化处理保证输出结果和为1,归一化就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比  

$$
softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^K e^{z_i}}
$$

其中$e^{z_i}$是第i个节点的输出，K为节点数，即总共的类别数

## Linear Regression
线性回归   
确定两种或两种以上变量间相互依赖的定量关系   
参考 https://zhuanlan.zhihu.com/p/147297924

最简单的线性回归：y=kx+b   
一般形式的：Y=WX+b

比一元线性回归复杂的是，多元线性回归组成的不是直线，是一个多维空间中的超平面，数据点散落在超平面两侧

## 最小二乘法 least square method
基于均方误差最小化来进行模型求解的方法。  
就是试图找到一条直线，使所有样本到该直线的欧氏距离之和最小

## Logistic Regression
逻辑回归

## Entropy 熵
熵，是无损编码事件信息的最小平均编码长度，可以理解为一种期望，n种等可能状态，需要的最小编码长度为$log_{2}N$    
或者说 熵是服从某一特定概率分布事件的理论最小平均编码长度
参考https://zhuanlan.zhihu.com/p/149186719

$$
Entropy=-\sum_{i}P(i)log_{2}P(i)
$$

### Cross Entropy 交叉熵

$$
H(P,Q)=-\sum_{i}^{}{P(i)\log_2Q(i)}
$$

交叉熵对比模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。   
因此，训练分类模型时，可以使用交叉熵作为损失函数   
其中Q是预估的概率分布，P是真实的概率分布

## binary crossentropy loss 二分类交叉熵

$$
Loss=-\frac{1}{output size}\sum_{i=1}^{output size}y_{i}\cdot log \hat y_{i}+(1-y_{i})\cdot log(1-\hat y_{i})
$$

每个i是相互独立的，一般用于多标签分类，yolov3的分类损失函数就是binary crossentropy
## Focal Loss
https://www.cnblogs.com/endlesscoding/p/12155588.html    
Focal loss是cross entropy loss的升级版，为了解决数据集中正负样本比例严重失衡的问题，降低了大量简单负样本在训练中所占的权重，   
更准确地说，是为了解决难分类样本(hard example)和易分类样本(easy example)不平衡的问题，数据集中拥有太多的easy example，虽然loss较小，但是数量巨大，   
累积起来依然大于hard example的loss，因此需要给hard example更大的权重   

$$
FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)
$$

![img1]({{site.usr}}/img/focalloss.png)

## darknet cfg文件中的一些参数
### batch
一个batch，也就是一次迭代有多少图片

### subdivisions
一个batch分成subdivisions份，GPU同时处理`batch/subdivisions`张图片，经过`batch`个图片后更新一次权重

### max_batches
训练一共进行`max_batches`个batch   
epoch=max_batches/(total_num_images/(batch*no. of gpu))   
关于max_batches和epoch的关系，     
https://github.com/AlexeyAB/darknet/issues/2962

### letter_box
将图片等比例缩放输入网络

### saturation
训练时随机改变图像饱和度

### exposure
训练时随机改变图像亮度
## darknet 代码中的一些notes
### struct layer中的值
`n`：对应cfg中的filters，即这一层有多少个卷积核   
`scales`：一个float数组，数组大小为n，如果是bn层，每个值都是1

### 函数fuse_conv_batchnorm(net)
将卷积层和bn层融合为一层，详见https://github.com/AlexeyAB/darknet#improvements-in-this-repository

### cv::Mat::step
对于图像来说   
step[0]=cols*channels;//一行元素所占size   
step[1]=channels;//一个元素所占size

### mat_to_image(cv::Mat mat)
将cv::Mat转为自定义的image类型，改变了像素的存储方式，cv::Mat按行存储，每一行依次存每个像素的每个通道的值，image按通道存储，每个通道按行存储，即依次存每个通道的所有像素  

## L1、L2 norm
norm通常用来表示两个向量的距离，定义为   
$$
||x||_p=(\sum_{i=1}^n|x_i|^p)^{1/p}
$$   

L1-norm，定义为   
$$
||x||_1=(\sum_{i=1}^n|x_i|)
$$

即所有元素绝对值的和   

L2-norm，定义为   
$$
||x||_2=(\sum_{i=1}^n|x_i|^2)^{1/2}=\sqrt{x_1^2 + x_2^2 + ... + x_n^2}
$$

也就是欧式距离

## TOPS,FLOPS,FLOPs的区别
**FLOPS**, 是每秒所执行的浮点运算次数，Floating-point Operations Per Second的缩写，用来表示处理器的性能，浮点运算比整数运算更复杂、更精确、更耗费时间
**TOPS**，是每秒所执行的操作次数， Tera Operations Per Second的缩写，1TOPS表示每秒可进行一万亿次操作。   
**FLOPs**，是指浮点运算数，floating point operations的缩写，可以理解为计算量，用来评估模型的复杂度。

FLOPS和TOPS是描述处理器的，FLOPs是描述算法模型的。

•  Transformer 模型介绍及与 RNN/LSTM 的区别   
Transformer 是一种基于自注意力机制的深度学习架构，它主要由编码器和解码器组成。每个编码器和解码器都由若干个相同的层构成，每层中都包含多头注意力和全连接的前馈网络。Transformer 的关键优势在于自注意力机制，它允许模型直接计算序列中任何两个位置之间的依赖关系，无需通过递归。与 RNN/LSTM 相比，Transformer 可以并行处理序列中的所有元素，这使得训练速度快得多。此外，Transformer 能够更好地处理长距离依赖问题，因为它不像 RNN 或 LSTM 那样受到梯度消失的限制。 

•  自注意力机制   
自注意力机制允许模型在处理一个序列时对其内部的不同部分进行加权。具体来说，这是通过计算序列中每个元素的点积（query 和 key 的点积）来实现的，再对点积进行缩放（除以 $\sqrt{d_k}$，其中 $d_k$ 是键向量的维度）。然后应用 softmax 函数，将这些数值转换成概率分布形式（attention weights）。这些概率用来加权相应的值（value）向量，最终的输出是这些加权的总和。通过这种机制，每个序列元素都能够在计算其表示时参考到整个序列的信息。  

•  位置编码   
由于 Transformer 中缺少任何对输入序列元素顺序的隐式建模（如递归网络中的时间步长），因此需要通过位置编码来给模型提供关于元素位置的信息。通常这种编码是通过将每个位置的正弦和余弦函数的值加到对应的嵌入向量中，这些正弦和余弦函数的频率遵循几何级数，这允许模型即使对于长序列也能区分不同位置。   

•  多头注意力   
在 Transformer 的上下文中，多头注意力机制指的是并行计算多组自注意力（即头）。每个“头”学习序列的不同方面，这样一来，模型可以同时在不同的子空间学习信息。实际上，这是通过为每个头使用不同的、可学习的线性变换（权重矩阵）来实现的，用以产生相应的 query、key 和 value。每个头的输出然后被拼接，并再次线性变换形成最终的输出。  

•  层归一化和残差连接   
层归一化是指对每一个子层输出的激活值沿特征维度进行归一化。这有助于稳定训练过程并加快收敛速度。残差连接，又称作残差跳过（skip connections），是通过将输入直接加到子层的输出上（在层归一化前），这使得即使是更深的网络，每层的梯度也能够更加稳定地传播并且减轻梯度消失的问题。  

•  Transformer 高效的原因   
由于 Transformer 模型依赖的是自注意力机制，这使得模型可以在处理数据时并行化操作，要知道 RNN 或 LSTM 必须按时间步骤顺序处理数据。此外，自注意力允许模型不受顺序长度限制地对所有位置之间的依赖进行建模，而传统的递归网络在长序列上常常面临着梯度消失或爆炸的问题。  

•  优化方法和训练技巧   
常见的优化方法包括使用 Adam 优化器，它结合了动量优化和 RMSProp 的优点，能自适应地调整每个参数的学习率。此外，Transformer 模型常采用自定义的学习率调度器，开始时采用学习率预热策略，在训练早期逐渐增加学习率，然后再逐步减少。此外，标签平滑是一种正则化技巧，它通过将标签值从 0 和 1 平滑到一个较小的范围内，避免模型对输出过于自信，增强泛化能力。  

•  改进和变体   
BERT 是一个在大量文本上预训练的双向 Transformer 编码器，它通过掩蔽语言模型（MLM）和下一句预测（NSP）的任务，利用了上下文两侧的信息，学习了深层次的语言特征。GPT 系列模型则采用了 Transformer 解码器，通过单向语境进行预训练，旨在推动文本生成任务的表现。这些变体捕捉到了不同方面的语言规律，并带来更好的性能。  

•  序列生成   
在机器翻译等序列生成任务中，Transformer 的解码器采用了逐步解码的方式，每次生成一个元素，并将其反馈为下一步的输入。解码过程常结合贪婪搜索，注意力机制，以及束搜索等方法以提高生成结果的质量。束搜索通过保持一定数量的最佳假设并在每个步骤中展开它们，而不是单一地选择概率最高的选项，以此平衡搜索质量与效率。  

•  防止过拟合   
为了防止过拟合，可采用不同的策略，如增加数据集规模、数据增强、引入 dropout（在训练时随机丢弃部分神经元），以及正则化方法。另外，还可以使用提早停止来避免在验证集上的性能不再提升时继续训练，从而防止在训练数据上过度拟合。  

•  内存和计算问题   
Transformer 处理特别长的序列时会遇到内存和计算需求增大的问题，因为自注意力的复杂度与序列长度的平方成正比。为解决这一问题，可以采用各种策略，例如梯度累积（分多个小批次累积梯度后再执行一次参数更新），降低批量大小（减少同步计算的序列数量），或使用16位浮点数进行混合精度训练。这能减少内存使用、加速训练，同时对模型精度的影响微乎其微。  

•  处理长序列   
对于 Transformer 模型，长序列的处理是一个挑战，特别是对于基于标准自注意力的原始架构。为了解决这一问题，研究人员开发了多种方法，如稀疏化注意力模型（仅在序列中计算一部分注意力权重，如 Reformer 或 Longformer），或者使用内存高效的变换技术，如使用局部窗口或者层次化的注意力。  

•  Transformer 不适宜的任务   
Transformer 模型可能不适合需要强烈的顺序处理或很短的处理时间的任务。例如，在在线的、低延迟的实时任务（如语音识别的增量解码）中，递归模型或许会是更好的选择。此外，在处理非常长的序列时，如果没有专门的调整，原始的 Transformer 可能效率低下。  

•  解释性   
尽管 Transformer 强大，但其决策过程往往比较难以解释。注意力权重的可视化是一种常见的方法，它能提供模型在计算每个新的输出元素时关注输入序列哪些部分的视觉线索。此外，可以使用一些工具和技术，如梯度类激活映射（Grad-CAM）或集成梯度，来识别模型做出某一预测的关键输入特征。  

•  部署考虑因素   
在不同的硬件配置上部署 Transformer 模型时需要考虑到诸如模型尺寸、延迟和吞吐量等方面。考虑到不同的设备可能有不同的内存和计算能力限制，可以采取的优化策略包括模型量化（将32位的浮点数转换为更低位的表示，如8位整数），模型剪枝（去除一些不重要的权重）以及知识蒸馏（训练一个更小的模型来模仿大模型的行为）。这些技术可以在不显著损失性能的情况下显著减少模型的大小和加速推理。

