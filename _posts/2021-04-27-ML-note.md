---
layout: post
title:  "ML-note"
date:   2021-04-27 10:16:26 +0800
categories: jekyll update
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
## softmax
softmax又称为归一化指数函数，目的是将多分类的结果以概率的形式展现出来，  
将每个节点的输入映射到0-1之间，且输出的值和为1  
使用指数函数保证输出结果非负，归一化处理保证输出结果和为1,归一化就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比  

$$
softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^K e^{z_i}}
$$

其中$e^{z_i}$是第i个节点的输出，K为节点数，即总共的类别数

## Linear Regression
线性回归   
确定两种或两种以上变量间相互依赖的定量关系   
参考 https://zhuanlan.zhihu.com/p/147297924

最简单的线性回归：y=kx+b   
一般形式的：Y=WX+b

比一元线性回归复杂的是，多元线性回归组成的不是直线，是一个多维空间中的超平面，数据点散落在超平面两侧

## Logistic Regression
逻辑回归

## Entropy
熵，是无损编码事件信息的最小平均编码长度，可以理解为一种期望，n种等可能状态，需要的最小编码长度为$log_{2}N$    
参考https://zhuanlan.zhihu.com/p/149186719

$$
Entropy=-\sum_{i}P(i)log_{2}P(i)
$$

## binary crossentropy loss

$$
Loss=-\frac{1}{output size}\sum_{i=1}^{output size}y_{i}\cdot log \hat y_{i}+(1-y_{i})\cdot log(1-\hat y_{i})
$$

每个i是相互独立的，一般用于多标签分类，yolov3的分类损失函数就是binary crossentropy