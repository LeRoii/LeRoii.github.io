---
layout: post
title:  "ML-note"
date:   2021-04-27 10:16:26 +0800
categories: jekyll update
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
## softmax
softmax又称为归一化指数函数，目的是将多分类的结果以概率的形式展现出来，  
将每个节点的输入映射到0-1之间，且输出的值和为1  
使用指数函数保证输出结果非负，归一化处理保证输出结果和为1,归一化就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比  

$$
softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^K e^{z_i}}
$$

其中$e^{z_i}$是第i个节点的输出，K为节点数，即总共的类别数

## Linear Regression
线性回归   
确定两种或两种以上变量间相互依赖的定量关系   
参考 https://zhuanlan.zhihu.com/p/147297924

最简单的线性回归：y=kx+b   
一般形式的：Y=WX+b

比一元线性回归复杂的是，多元线性回归组成的不是直线，是一个多维空间中的超平面，数据点散落在超平面两侧

## 最小二乘法 least square method
基于均方误差最小化来进行模型求解的方法。  
就是试图找到一条直线，使所有样本到该直线的欧氏距离之和最小

## Logistic Regression
逻辑回归

## Entropy 熵
熵，是无损编码事件信息的最小平均编码长度，可以理解为一种期望，n种等可能状态，需要的最小编码长度为$log_{2}N$    
或者说 熵是服从某一特定概率分布事件的理论最小平均编码长度
参考https://zhuanlan.zhihu.com/p/149186719

$$
Entropy=-\sum_{i}P(i)log_{2}P(i)
$$

### Cross Entropy 交叉熵

$$
H(P,Q)=-\sum_{i}^{}{P(i)\log_2Q(i)}
$$

交叉熵对比模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。   
因此，训练分类模型时，可以使用交叉熵作为损失函数   
其中Q是预估的概率分布，P是真实的概率分布

## binary crossentropy loss 二分类交叉熵

$$
Loss=-\frac{1}{output size}\sum_{i=1}^{output size}y_{i}\cdot log \hat y_{i}+(1-y_{i})\cdot log(1-\hat y_{i})
$$

每个i是相互独立的，一般用于多标签分类，yolov3的分类损失函数就是binary crossentropy
## Focal Loss
https://www.cnblogs.com/endlesscoding/p/12155588.html    
Focal loss是cross entropy loss的升级版，为了解决数据集中正负样本比例严重失衡的问题，降低了大量简单负样本在训练中所占的权重，   
更准确地说，是为了解决难分类样本(hard example)和易分类样本(easy example)不平衡的问题，数据集中拥有太多的easy example，虽然loss较小，但是数量巨大，   
累积起来依然大于hard example的loss，因此需要给hard example更大的权重   

$$
FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)
$$

![img1]({{site.usr}}/img/focalloss.png)

## darknet cfg文件中的一些参数
### batch
一个batch，也就是一次迭代有多少图片

### subdivisions
一个batch分成subdivisions份，GPU同时处理`batch/subdivisions`张图片，经过`batch`个图片后更新一次权重

### max_batches
训练一共进行`max_batches`个batch   
epoch=max_batches/(total_num_images/(batch*no. of gpu))   
关于max_batches和epoch的关系，     
https://github.com/AlexeyAB/darknet/issues/2962

### letter_box
将图片等比例缩放输入网络

### saturation
训练时随机改变图像饱和度

### exposure
训练时随机改变图像亮度
## darknet 代码中的一些notes
### struct layer中的值
`n`：对应cfg中的filters，即这一层有多少个卷积核   
`scales`：一个float数组，数组大小为n，如果是bn层，每个值都是1

### 函数fuse_conv_batchnorm(net)
将卷积层和bn层融合为一层，详见https://github.com/AlexeyAB/darknet#improvements-in-this-repository

### cv::Mat::step
对于图像来说   
step[0]=cols*channels;//一行元素所占size   
step[1]=channels;//一个元素所占size

### mat_to_image(cv::Mat mat)
将cv::Mat转为自定义的image类型，改变了像素的存储方式，cv::Mat按行存储，每一行依次存每个像素的每个通道的值，image按通道存储，每个通道按行存储，即依次存每个通道的所有像素  

## L1、L2 norm
norm通常用来表示两个向量的距离，定义为   
$$
||x||_p=(\sum_{i=1}^n|x_i|^p)^{1/p}
$$   

L1-norm，定义为   
$$
||x||_1=(\sum_{i=1}^n|x_i|)
$$

即所有元素绝对值的和   

L2-norm，定义为   
$$
||x||_2=(\sum_{i=1}^n|x_i|^2)^{1/2}=\sqrt{x_1^2 + x_2^2 + ... + x_n^2}
$$

也就是欧式距离

## TOPS,FLOPS,FLOPs的区别
**FLOPS**, 是每秒所执行的浮点运算次数，Floating-point Operations Per Second的缩写，用来表示处理器的性能，浮点运算比整数运算更复杂、更精确、更耗费时间
**TOPS**，是每秒所执行的操作次数， Tera Operations Per Second的缩写，1TOPS表示每秒可进行一万亿次操作。   
**FLOPs**，是指浮点运算数，floating point operations的缩写，可以理解为计算量，用来评估模型的复杂度。

FLOPS和TOPS是描述处理器的，FLOPs是描述算法模型的。