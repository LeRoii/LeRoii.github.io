---
layout: post
title:  "ML-note"
date:   2021-04-27 10:16:26 +0800
categories: jekyll update
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
## softmax
softmax又称为归一化指数函数，目的是将多分类的结果以概率的形式展现出来，  
将每个节点的输入映射到0-1之间，且输出的值和为1  
使用指数函数保证输出结果非负，归一化处理保证输出结果和为1,归一化就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比  

$$
softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^K e^{z_i}}
$$

其中$e^{z_i}$是第i个节点的输出，K为节点数，即总共的类别数

## Linear Regression
线性回归   
确定两种或两种以上变量间相互依赖的定量关系   
参考 https://zhuanlan.zhihu.com/p/147297924

最简单的线性回归：y=kx+b   
一般形式的：Y=WX+b

比一元线性回归复杂的是，多元线性回归组成的不是直线，是一个多维空间中的超平面，数据点散落在超平面两侧

## Logistic Regression
逻辑回归

## Entropy 熵
熵，是无损编码事件信息的最小平均编码长度，可以理解为一种期望，n种等可能状态，需要的最小编码长度为$log_{2}N$    
或者说 熵是服从某一特定概率分布事件的理论最小平均编码长度
参考https://zhuanlan.zhihu.com/p/149186719

$$
Entropy=-\sum_{i}P(i)log_{2}P(i)
$$

### Cross Entropy 交叉熵

$$
H(P,Q)=-\sum_{i}^{}{P(i)\log_2Q(i)}
$$

交叉熵对比模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。   
因此，训练分类模型时，可以使用交叉熵作为损失函数   
其中Q是预估的概率分布，P是真实的概率分布

## binary crossentropy loss 二分类交叉熵
$$
Loss=-\frac{1}{output size}\sum_{i=1}^{output size}y_{i}\cdot log \hat y_{i}+(1-y_{i})\cdot log(1-\hat y_{i})
$$
每个i是相互独立的，一般用于多标签分类，yolov3的分类损失函数就是binary crossentropy
## Focal Loss
https://www.cnblogs.com/endlesscoding/p/12155588.html    
Focal loss是cross entropy loss的升级版，为了解决数据集中正负样本比例严重失衡的问题，降低了大量简单负样本在训练中所占的权重，   
更准确地说，是为了解决难分类样本(hard example)和易分类样本(easy example)不平衡的问题，数据集中拥有太多的easy example，虽然loss较小，但是数量巨大，   
累积起来依然大于hard example的loss，因此需要给hard example更大的权重   
$$
FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)
$$
![img1]({{site.usr}}/img/focalloss.png)